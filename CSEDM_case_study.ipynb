{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSEDM Data Case study\n",
    "base: uirt_lfa_model_csedm_final.ipynb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ProgSnap2 import ProgSnap2Dataset\n",
    "from ProgSnap2 import PS2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from os import path\n",
    "import sys\n",
    "sys.path.append(os.path.realpath(\"./models\"))\n",
    "sys.path.append(os.path.realpath(\"./em_algorithm\"))\n",
    "# Custom modules, import violates pep8, so we have to declare an exeption\n",
    "if True:  # noqa: E402\n",
    "    from mirt_2pl import mirt_2pl\n",
    "    from mirt_2pl_gain import mirt_2pl_gain\n",
    "    from e_step_mirt_2pl import e_step_ga_mml\n",
    "    from e_step_mirt_2pl_gain import e_step_ga_mml_gain\n",
    "    from m_step_mirt_2pl import m_step_ga_mml\n",
    "    from m_step_mirt_2pl_gain import m_step_ga_mml_gain\n",
    "    from em_algorithm import em_algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "semester = 'F19'\n",
    "BASE_PATH = os.path.join('path_to_dataset/csedm_challenge_dataset', 'F19_Release_Train_06-28-21')\n",
    "TRAIN_PATH = os.path.join(BASE_PATH, 'Train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset for early Problem Performance (Aggregated from Event-Data)\n",
    "early = pd.read_csv(os.path.join(TRAIN_PATH, 'early.csv'))\n",
    "early[\"alt_Label\"] = early.apply(lambda x: 1 if (x[\"CorrectEventually\"] and x[\"Attempts\"] <= 3) else 0, axis=1)\n",
    "early\n",
    "\n",
    "late = pd.read_csv(os.path.join(TRAIN_PATH, 'late.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "#Obtain Matrix of Response-vectors\n",
    "U = pd.crosstab(index = early[\"SubjectID\"], \n",
    "            columns=[early[\"AssignmentID\"], early[\"ProblemID\"]], \n",
    "            values=early[\"Label\"],\n",
    "            aggfunc=\"sum\")\n",
    "#At the moment we will delete missing values row-wise. It will be usefull to impute values later, since missing at random is unlikely\n",
    "imputer = KNNImputer(n_neighbors=4)\n",
    "U = imputer.fit_transform(U).round()\n",
    "U = pd.DataFrame(U)\n",
    "\n",
    "#Obtain Matrix of Response-vectors\n",
    "V = pd.crosstab(index = late[\"SubjectID\"], \n",
    "            columns=[late[\"AssignmentID\"], late[\"ProblemID\"]], \n",
    "            values=late[\"Label\"],\n",
    "            aggfunc=\"sum\")\n",
    "#At the moment we will delete missing values row-wise. It will be usefull to impute values later, since missing at random is unlikely\n",
    "#late_U = late_U.dropna()\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=4)\n",
    "V = imputer.fit_transform(V).round()\n",
    "V = pd.DataFrame(V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>367 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1    2    3    4    5    6    7    8    9    10   11   12   13  \\\n",
       "0    1.0  1.0  1.0  1.0  1.0  1.0  1.0  0.0  1.0  0.0  1.0  1.0  0.0  1.0   \n",
       "1    1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  1.0  0.0   \n",
       "2    0.0  0.0  1.0  0.0  0.0  0.0  1.0  1.0  0.0  1.0  0.0  0.0  0.0  1.0   \n",
       "3    1.0  1.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0   \n",
       "4    1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0   \n",
       "..   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "362  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0   \n",
       "363  0.0  1.0  1.0  0.0  1.0  1.0  0.0  0.0  0.0  1.0  0.0  0.0  1.0  1.0   \n",
       "364  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0   \n",
       "365  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0   \n",
       "366  1.0  1.0  1.0  1.0  1.0  1.0  0.0  0.0  1.0  0.0  1.0  1.0  0.0  1.0   \n",
       "\n",
       "      14   15   16   17   18   19  \n",
       "0    1.0  0.0  0.0  1.0  1.0  1.0  \n",
       "1    1.0  1.0  0.0  1.0  1.0  1.0  \n",
       "2    0.0  1.0  0.0  1.0  1.0  1.0  \n",
       "3    1.0  0.0  0.0  0.0  0.0  1.0  \n",
       "4    1.0  1.0  1.0  1.0  1.0  1.0  \n",
       "..   ...  ...  ...  ...  ...  ...  \n",
       "362  1.0  1.0  1.0  1.0  1.0  1.0  \n",
       "363  1.0  1.0  1.0  1.0  1.0  1.0  \n",
       "364  1.0  1.0  1.0  0.0  0.0  1.0  \n",
       "365  1.0  1.0  1.0  1.0  1.0  1.0  \n",
       "366  1.0  1.0  0.0  1.0  1.0  1.0  \n",
       "\n",
       "[367 rows x 20 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['AssignmentID', 'ProblemID', 'Requirement', 'If/Else', 'NestedIf',\n",
       "       'While', 'For', 'NestedFor', 'Math+-*/', 'Math%', 'LogicAndNotOr',\n",
       "       'LogicCompareNum', 'LogicBoolean', 'StringFormat', 'StringConcat',\n",
       "       'StringIndex', 'StringLen', 'StringEqual', 'CharEqual', 'ArrayIndex',\n",
       "       'DefFunction'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Source: https://docs.google.com/spreadsheets/d/1-APxPAVP3PylM0fskzDNgpalj1DTuMqDLkgBC5QxJdg/edit#gid=0\n",
    "item_skill_df = pd.read_csv(\"CSEDM Challenge/item_skill_relation.csv\")\n",
    "item_skill_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "logic_skills = ['If/Else', 'NestedIf', 'LogicAndNotOr', 'LogicCompareNum', 'LogicBoolean']\n",
    "string_skills = ['StringFormat', 'StringConcat', 'StringIndex', 'StringLen', 'StringEqual', 'CharEqual']\n",
    "iter_skills = ['While', 'For', 'NestedFor', 'ArrayIndex']\n",
    "\n",
    "logic_exercises = (np.sum(item_skill_df[logic_skills], axis=1) > 0)\n",
    "string_exersises = (np.sum(item_skill_df[string_skills], axis=1) > 0)\n",
    "iter_exercises = (np.sum(item_skill_df[iter_skills], axis=1) > 0)\n",
    "\n",
    "full_Q = np.stack((logic_exercises, iter_exercises), axis=1)\n",
    "early_Q = full_Q[0:30,:]\n",
    "late_Q = full_Q[30:50, :]\n",
    "# Ensure Identification\n",
    "late_Q[2,1] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False,  True],\n",
       "       [ True,  True],\n",
       "       [ True, False],\n",
       "       [ True,  True],\n",
       "       [ True,  True],\n",
       "       [ True,  True],\n",
       "       [ True,  True],\n",
       "       [ True,  True],\n",
       "       [ True,  True],\n",
       "       [ True,  True],\n",
       "       [ True,  True],\n",
       "       [ True,  True],\n",
       "       [ True,  True],\n",
       "       [ True,  True],\n",
       "       [False,  True],\n",
       "       [ True,  True],\n",
       "       [ True,  True],\n",
       "       [ True,  True],\n",
       "       [False,  True],\n",
       "       [False,  True]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "late_Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(100)\n",
    "np.random.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EM Iteration 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jesper\\anaconda3\\lib\\site-packages\\scipy\\stats\\_qmc.py:2039: UserWarning: The balance properties of Sobol' points require n to be a power of 2.\n",
      "  samples = self.engine.random(n)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Monte Carlo Sample size: 300\n",
      "[[1.     0.3948]\n",
      " [0.3948 0.9999]]\n",
      "Step: 2: current parameter_diff: 7.123144369817095, current marginal loglikelihood: -5613.442061953425\n",
      "EM Iteration 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jesper\\anaconda3\\lib\\site-packages\\scipy\\stats\\_qmc.py:2039: UserWarning: The balance properties of Sobol' points require n to be a power of 2.\n",
      "  samples = self.engine.random(n)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Monte Carlo Sample size: 324\n",
      "[[1.     0.3537]\n",
      " [0.3537 1.    ]]\n",
      "Step: 3: current parameter_diff: 3.725735565101794, current marginal loglikelihood: -5599.26968885875\n",
      "EM Iteration 4\n",
      "Current Monte Carlo Sample size: 324\n",
      "[[0.9999 0.3098]\n",
      " [0.3098 1.    ]]\n",
      "Step: 4: current parameter_diff: 3.638039734747279, current marginal loglikelihood: -5586.116490732511\n",
      "EM Iteration 5\n",
      "Current Monte Carlo Sample size: 349\n",
      "[[1.     0.2839]\n",
      " [0.2839 1.    ]]\n",
      "Step: 5: current parameter_diff: 2.258091755157589, current marginal loglikelihood: -5582.862166434623\n",
      "EM Iteration 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jesper\\anaconda3\\lib\\site-packages\\scipy\\stats\\_qmc.py:2039: UserWarning: The balance properties of Sobol' points require n to be a power of 2.\n",
      "  samples = self.engine.random(n)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Monte Carlo Sample size: 349\n",
      "[[0.9999 0.2917]\n",
      " [0.2917 1.    ]]\n",
      "Step: 6: current parameter_diff: 1.0753392824248909, current marginal loglikelihood: -5579.15852880436\n",
      "EM Iteration 7\n",
      "Current Monte Carlo Sample size: 349\n",
      "[[1.     0.3025]\n",
      " [0.3025 1.    ]]\n",
      "Step: 7: current parameter_diff: 0.7062163531582382, current marginal loglikelihood: -5584.9940014358435\n",
      "EM Iteration 8\n",
      "Current Monte Carlo Sample size: 349\n",
      "[[1.     0.3094]\n",
      " [0.3094 1.    ]]\n",
      "Step: 8: current parameter_diff: 0.8665951325205431, current marginal loglikelihood: -5580.383059828765\n",
      "EM Iteration 9\n",
      "Current Monte Carlo Sample size: 349\n",
      "[[1.     0.2795]\n",
      " [0.2795 1.    ]]\n",
      "Step: 9: current parameter_diff: 0.43904620265697614, current marginal loglikelihood: -5580.214142885234\n",
      "EM Iteration 10\n",
      "Current Monte Carlo Sample size: 349\n",
      "[[0.9999 0.2841]\n",
      " [0.2841 0.9999]]\n",
      "Step: 10: current parameter_diff: 1.3276140805840662, current marginal loglikelihood: -5574.9397713595\n"
     ]
    }
   ],
   "source": [
    "# Fit early Model\n",
    "early_model = mirt_2pl(latent_dimension=2, item_dimension=U.shape[1], Q=early_Q)\n",
    "early_model.initialize_from_responses(response_data=U)\n",
    "e_step = e_step_ga_mml(model=early_model)\n",
    "m_step = m_step_ga_mml(\n",
    "    early_model, sigma_constraint=\"early_constraint\")\n",
    "em = em_algo(e_step=e_step, m_step=m_step, model=early_model)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "em.fit([U.reset_index(drop=True)], max_iter=30,\n",
    "        stop_threshold=0.001, person_method=\"BFGS\")\n",
    "run_time = (time.time() - start_time)\n",
    "theta_hat = early_model.predict_competency(U)\n",
    "estimated_early_item_parameters = em.model.item_parameters\n",
    "estimated_early_person_parameters = em.model.person_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early logit variance: 1.2907867467786318\n",
      "Late logit variance: 1.790658381505874\n",
      "Approximate summation variance: 1.3872612079220314\n"
     ]
    }
   ],
   "source": [
    "p_early = np.mean(U, axis=1)\n",
    "p_late = np.mean(V, axis=1)\n",
    "# Correct numerical problems with the inverse logistic function\n",
    "p_early[p_early == 0] = np.min(p_early[p_early != 0])\n",
    "p_early[p_early == 1] = np.max(p_early[p_early != 1])\n",
    "p_late[p_late == 0] = np.min(p_late[p_late != 0])\n",
    "p_late[p_late == 1] = np.max(p_late[p_late != 1])\n",
    "# get logits\n",
    "logit_early = np.log(np.divide(p_early, 1 -\n",
    "                                p_early))\n",
    "logit_late = np.log(np.divide(p_late, 1 -\n",
    "                                p_late))\n",
    "var_logit_early = np.var(logit_early)\n",
    "var_logit_late = np.var(logit_late)\n",
    "\n",
    "#\n",
    "print(\"Early logit variance: {0}\".format(var_logit_early))\n",
    "print(\"Late logit variance: {0}\".format(var_logit_late))\n",
    "print(\"Approximate summation variance: {0}\".format(var_logit_late/var_logit_early))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EM Iteration 2\n",
      "Current Monte Carlo Sample size: 300\n",
      "[[0.9999 0.2841 0.1303 0.0763]\n",
      " [0.2841 0.9999 0.0727 0.1302]\n",
      " [0.1303 0.0727 0.3484 0.1035]\n",
      " [0.0763 0.1302 0.1035 0.3393]]\n",
      "Step: 2: current parameter_diff: 16.876094476988214, current marginal loglikelihood: -4293.1824504578635\n",
      "EM Iteration 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jesper\\anaconda3\\lib\\site-packages\\scipy\\stats\\_qmc.py:2039: UserWarning: The balance properties of Sobol' points require n to be a power of 2.\n",
      "  samples = self.engine.random(n)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Monte Carlo Sample size: 324\n",
      "[[0.9999 0.2841 0.1338 0.0739]\n",
      " [0.2841 0.9999 0.0702 0.1393]\n",
      " [0.1338 0.0702 0.3553 0.1025]\n",
      " [0.0739 0.1393 0.1025 0.345 ]]\n",
      "Step: 3: current parameter_diff: 6.883042924705552, current marginal loglikelihood: -4271.056041684078\n",
      "EM Iteration 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jesper\\anaconda3\\lib\\site-packages\\scipy\\stats\\_qmc.py:2039: UserWarning: The balance properties of Sobol' points require n to be a power of 2.\n",
      "  samples = self.engine.random(n)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Monte Carlo Sample size: 349\n",
      "[[0.9999 0.2841 0.1336 0.0723]\n",
      " [0.2841 0.9999 0.0697 0.1364]\n",
      " [0.1336 0.0697 0.3554 0.0998]\n",
      " [0.0723 0.1364 0.0998 0.3474]]\n",
      "Step: 4: current parameter_diff: 2.840741457142374, current marginal loglikelihood: -4255.037156020615\n",
      "EM Iteration 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jesper\\anaconda3\\lib\\site-packages\\scipy\\stats\\_qmc.py:2039: UserWarning: The balance properties of Sobol' points require n to be a power of 2.\n",
      "  samples = self.engine.random(n)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Monte Carlo Sample size: 376\n",
      "[[0.9999 0.2841 0.132  0.0719]\n",
      " [0.2841 0.9999 0.073  0.1381]\n",
      " [0.132  0.073  0.3581 0.1028]\n",
      " [0.0719 0.1381 0.1028 0.3456]]\n",
      "Step: 5: current parameter_diff: 3.9761194069755814, current marginal loglikelihood: -4243.3974919623215\n",
      "EM Iteration 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jesper\\anaconda3\\lib\\site-packages\\scipy\\stats\\_qmc.py:2039: UserWarning: The balance properties of Sobol' points require n to be a power of 2.\n",
      "  samples = self.engine.random(n)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Monte Carlo Sample size: 406\n",
      "[[0.9999 0.2841 0.1299 0.0713]\n",
      " [0.2841 0.9999 0.0746 0.1369]\n",
      " [0.1299 0.0746 0.3637 0.107 ]\n",
      " [0.0713 0.1369 0.107  0.3478]]\n",
      "Step: 6: current parameter_diff: 1.221869340307339, current marginal loglikelihood: -4241.188728135961\n"
     ]
    }
   ],
   "source": [
    "#Fit late Model\n",
    "# Initialize Model\n",
    "late_model = mirt_2pl_gain(item_dimension=20, latent_dimension=2, mu=np.zeros(2),\n",
    "                            early_sigma=estimated_early_person_parameters[\"covariance\"],\n",
    "                            Q=late_Q)\n",
    "# TODO: Check if theta_hat can be used\n",
    "late_model.initialize_from_responses(\n",
    "    late_response_data=V, early_response_data=U,\n",
    "    convolution_variance=np.ones(2)*var_logit_late/var_logit_early,\n",
    "    sigma=False)\n",
    "late_initial_parameters = late_model.get_parameters()\n",
    "e_step = e_step_ga_mml_gain(\n",
    "    model=late_model, convolution_factor=20, gamma=1.5)\n",
    "m_step = m_step_ga_mml_gain(\n",
    "    late_model, sigma_constraint=\"early_constraint\")\n",
    "em = em_algo(e_step=e_step, m_step=m_step, model=late_model)\n",
    "\n",
    "# Fit late model\n",
    "start_time = time.time()\n",
    "em.fit([V, pd.DataFrame(theta_hat)], max_iter=30,\n",
    "        stop_threshold=0.005, person_method=\"BFGS\")\n",
    "run_time = (time.time() - start_time)\n",
    "s_hat = late_model.predict_gain(\n",
    "    V, pd.DataFrame(theta_hat))\n",
    "\n",
    "s_pred_train = late_model.predict_gain(\n",
    "    theta=pd.DataFrame(theta_hat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postprocess\n",
    "\n",
    "#### Item Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Item Parameters \n",
    "\n",
    "# Early \n",
    "early_A = early_model.item_parameters[\"discrimination_matrix\"]\n",
    "early_delta = early_model.item_parameters[\"intercept_vector\"]\n",
    "early_Q = early_model.item_parameters[\"q_matrix\"]\n",
    "\n",
    "# Late\n",
    "late_A = late_model.item_parameters[\"discrimination_matrix\"]\n",
    "late_delta = late_model.item_parameters[\"intercept_vector\"]\n",
    "late_Q = late_model.item_parameters[\"q_matrix\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rel_test_difficulty(A, delta):\n",
    "    rel_item_difficulty = np.divide(np.expand_dims(-delta, 1), A)\n",
    "    rel_item_difficulty[np.isinf(rel_item_difficulty)] = np.nan\n",
    "    rel_test_difficulty = np.nanmedian(rel_item_difficulty, axis=0)\n",
    "    return(rel_test_difficulty)\n",
    "\n",
    "\n",
    "def shift_table(s_hat, A, delta, early_reldiff):\n",
    "    quantiles = np.arange(0, 0.5, 0.01)\n",
    "    perc_pos_logic =[]\n",
    "    perc_pos_iter = []\n",
    "    logic_reldiff = []\n",
    "    iter_reldiff = []\n",
    "    for quantile in quantiles:\n",
    "        shift = -1*np.quantile(s_hat, q=quantile, axis=0)\n",
    "        delta_shift = late_delta - np.dot(late_A, shift)\n",
    "        s_shift = s_hat + shift\n",
    "        perc_positive = np.divide(np.sum(s_shift >= 0, axis=0), np.sum(s_shift == s_shift, axis=0))\n",
    "        perc_pos_logic.append(perc_positive[0])\n",
    "        perc_pos_iter.append(perc_positive[1])\n",
    "        rel_test_diff = rel_test_difficulty(A=A, delta=delta_shift)\n",
    "        logic_reldiff.append(rel_test_diff[0])\n",
    "        iter_reldiff.append(rel_test_diff[1])\n",
    "# Create two subplots and unpack the output array immediately\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
    "    f.set_figwidth(9)\n",
    "    f.tight_layout(pad=1.5)\n",
    "    #f.suptitle(\"Share of students with Competency \\n Gain vs. Relative Test Difficulty\", fontsize=14, y=1.05)\n",
    "    ax1.plot(perc_pos_logic, logic_reldiff)\n",
    "    ax1.plot(perc_pos_logic, [early_reldiff[0] for i in range(0, len(perc_pos_iter))])\n",
    "    ax1.set_title(\"Logic Competency\")\n",
    "    ax2.plot(perc_pos_iter, iter_reldiff)\n",
    "    ax2.plot(perc_pos_iter, [early_reldiff[1] for i in range(0, len(perc_pos_iter))])\n",
    "    ax2.set_title(\"Iteration Competency\")\n",
    "    ax1.set_ylabel(\"Relative Difficulty\", fontsize=12)\n",
    "    ax1.set_xlabel(\"Proportion of positive change (gain)\", fontsize=11)\n",
    "    ax2.set_xlabel(\"Proportion of positive change (gain)\", fontsize=11)\n",
    "    plt.legend([\"Late Difficulty\", \"Early Difficulty\"], fontsize=11)\n",
    "    #f.title(\"Share of students with Competency \\n Gain vs. Relative Test Difficulty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jesper\\AppData\\Local\\Temp\\ipykernel_17736\\1637136352.py:2: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  rel_item_difficulty = np.divide(np.expand_dims(-delta, 1), A)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 648x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "early_reldiff = rel_test_difficulty(A=early_A, delta=early_delta)\n",
    "shift_table(s_hat=s_hat, A=late_A, delta=late_delta, early_reldiff=early_reldiff)\n",
    "plt.savefig(\"results/plots/csedm_reldiff_vs_shift\", dpi=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gerha\\AppData\\Local\\Temp\\ipykernel_11312\\1158090350.py:2: RuntimeWarning: divide by zero encountered in divide\n",
      "  rel_item_difficulty = np.divide(np.expand_dims(-delta, 1), A)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.19815747, -0.6331392 ])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_test_difficulty(A=early_A, delta=early_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift = np.zeros(2)\n",
    "#Logic shift\n",
    "shift[0] = -1*np.quantile(s_hat[:,0], q=0.15, axis=0)\n",
    "#Iteration shift\n",
    "shift[1] = -1*np.quantile(s_hat[:,1], q=0.05, axis=0)\n",
    "s_hat_shift = s_hat + shift\n",
    "mu_shift = late_model.person_parameters[\"mean\"][2:4] + shift\n",
    "late_delta_shift = late_delta - np.dot(late_A, shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.85013624, 0.94822888])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check shift results\n",
    "np.divide(np.sum(s_hat_shift >= 0, axis=0), np.sum(s_hat_shift == s_hat_shift, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report Item Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_parameter_table(A, Q, delta, latent_description=[\"Logic\", \"Iteration\"], caption=\"\", n=367):\n",
    "    data = pd.DataFrame(np.concatenate((Q, A), axis=1))\n",
    "    data.columns = pd.MultiIndex.from_product([['Q', 'A'], latent_description],\n",
    "                                     names=['Param.', 'Latent dim.'])\n",
    "    delta_df = pd.DataFrame(np.expand_dims(delta, 1), columns=[(\"$\\Delta$\", \"\")])\n",
    "    data = pd.concat((data, delta_df), axis=1)\n",
    "    data = np.round(data, 3)\n",
    "    print(data.to_latex(bold_rows=True, multicolumn=True, \n",
    "                                   caption=caption+\", n={0}\".format(n), \n",
    "                                   position=\"H\", escape=False))\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[H]\n",
      "\\centering\n",
      "\\caption{CSEDM: Early Item Parameters, n=367}\n",
      "\\begin{tabular}{lrrrrr}\n",
      "\\toprule\n",
      "\\textbf{Param.} & \\multicolumn{2}{l}{Q} & \\multicolumn{2}{l}{A} & $\\Delta$ \\\\\n",
      "\\textbf{Latent dim.} & Logic & Iteration &  Logic & \\multicolumn{2}{l}{Iteration} \\\\\n",
      "\\midrule\n",
      "\\textbf{0 } &   1.0 &       0.0 &  0.834 &     0.000 &    1.637 \\\\\n",
      "\\textbf{1 } &   1.0 &       0.0 &  1.201 &     0.000 &    1.333 \\\\\n",
      "\\textbf{2 } &   1.0 &       0.0 &  0.877 &     0.000 &    1.268 \\\\\n",
      "\\textbf{3 } &   1.0 &       0.0 &  0.761 &     0.000 &    1.343 \\\\\n",
      "\\textbf{4 } &   1.0 &       0.0 &  1.182 &     0.000 &    1.407 \\\\\n",
      "\\textbf{5 } &   1.0 &       0.0 &  1.419 &     0.000 &    1.518 \\\\\n",
      "\\textbf{6 } &   1.0 &       0.0 &  0.973 &     0.000 &    1.128 \\\\\n",
      "\\textbf{7 } &   1.0 &       0.0 &  0.877 &     0.000 &    1.324 \\\\\n",
      "\\textbf{8 } &   1.0 &       0.0 &  0.953 &     0.000 &    1.170 \\\\\n",
      "\\textbf{9 } &   1.0 &       0.0 &  1.361 &     0.000 &    1.304 \\\\\n",
      "\\textbf{10} &   1.0 &       0.0 &  1.119 &     0.000 &    1.510 \\\\\n",
      "\\textbf{11} &   1.0 &       0.0 &  1.157 &     0.000 &    1.586 \\\\\n",
      "\\textbf{12} &   1.0 &       0.0 &  1.209 &     0.000 &    1.543 \\\\\n",
      "\\textbf{13} &   1.0 &       0.0 &  1.324 &     0.000 &    1.470 \\\\\n",
      "\\textbf{14} &   1.0 &       0.0 &  1.063 &     0.000 &    1.184 \\\\\n",
      "\\textbf{15} &   1.0 &       0.0 &  1.164 &     0.000 &    0.803 \\\\\n",
      "\\textbf{16} &   1.0 &       0.0 &  1.165 &     0.000 &    1.255 \\\\\n",
      "\\textbf{17} &   1.0 &       0.0 &  1.008 &     0.000 &    1.236 \\\\\n",
      "\\textbf{18} &   1.0 &       1.0 &  1.000 &     0.628 &    0.370 \\\\\n",
      "\\textbf{19} &   1.0 &       0.0 &  0.857 &     0.000 &    0.625 \\\\\n",
      "\\textbf{20} &   0.0 &       1.0 &  0.000 &     2.499 &    1.620 \\\\\n",
      "\\textbf{21} &   1.0 &       1.0 &  0.106 &     2.511 &    1.352 \\\\\n",
      "\\textbf{22} &   1.0 &       1.0 &  0.332 &     1.744 &    1.198 \\\\\n",
      "\\textbf{23} &   1.0 &       1.0 &  0.214 &     1.658 &    1.117 \\\\\n",
      "\\textbf{24} &   1.0 &       1.0 &  0.867 &     1.231 &    1.554 \\\\\n",
      "\\textbf{25} &   1.0 &       0.0 &  0.788 &     0.000 &    1.135 \\\\\n",
      "\\textbf{26} &   1.0 &       1.0 &  0.483 &     1.167 &    0.639 \\\\\n",
      "\\textbf{27} &   1.0 &       1.0 &  0.834 &     1.212 &    1.243 \\\\\n",
      "\\textbf{28} &   1.0 &       1.0 &  0.720 &     0.828 &    0.924 \\\\\n",
      "\\textbf{29} &   1.0 &       1.0 &  0.697 &     1.532 &    1.032 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jesper\\AppData\\Local\\Temp\\ipykernel_17736\\621918068.py:8: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(data.to_latex(bold_rows=True, multicolumn=True,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Param.</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Q</th>\n",
       "      <th colspan=\"2\" halign=\"left\">A</th>\n",
       "      <th>$\\Delta$</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Latent dim.</th>\n",
       "      <th>Logic</th>\n",
       "      <th>Iteration</th>\n",
       "      <th>Logic</th>\n",
       "      <th>Iteration</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.834</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.201</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.877</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.761</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.182</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.419</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.877</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.953</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.361</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.119</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.157</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.209</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.324</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.063</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.164</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.165</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.628</td>\n",
       "      <td>0.370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.857</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.499</td>\n",
       "      <td>1.620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.106</td>\n",
       "      <td>2.511</td>\n",
       "      <td>1.352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.332</td>\n",
       "      <td>1.744</td>\n",
       "      <td>1.198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.214</td>\n",
       "      <td>1.658</td>\n",
       "      <td>1.117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.867</td>\n",
       "      <td>1.231</td>\n",
       "      <td>1.554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.788</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.483</td>\n",
       "      <td>1.167</td>\n",
       "      <td>0.639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.834</td>\n",
       "      <td>1.212</td>\n",
       "      <td>1.243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.720</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.697</td>\n",
       "      <td>1.532</td>\n",
       "      <td>1.032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Param.          Q                A           $\\Delta$\n",
       "Latent dim. Logic Iteration  Logic Iteration         \n",
       "0             1.0       0.0  0.834     0.000    1.637\n",
       "1             1.0       0.0  1.201     0.000    1.333\n",
       "2             1.0       0.0  0.877     0.000    1.268\n",
       "3             1.0       0.0  0.761     0.000    1.343\n",
       "4             1.0       0.0  1.182     0.000    1.407\n",
       "5             1.0       0.0  1.419     0.000    1.518\n",
       "6             1.0       0.0  0.973     0.000    1.128\n",
       "7             1.0       0.0  0.877     0.000    1.324\n",
       "8             1.0       0.0  0.953     0.000    1.170\n",
       "9             1.0       0.0  1.361     0.000    1.304\n",
       "10            1.0       0.0  1.119     0.000    1.510\n",
       "11            1.0       0.0  1.157     0.000    1.586\n",
       "12            1.0       0.0  1.209     0.000    1.543\n",
       "13            1.0       0.0  1.324     0.000    1.470\n",
       "14            1.0       0.0  1.063     0.000    1.184\n",
       "15            1.0       0.0  1.164     0.000    0.803\n",
       "16            1.0       0.0  1.165     0.000    1.255\n",
       "17            1.0       0.0  1.008     0.000    1.236\n",
       "18            1.0       1.0  1.000     0.628    0.370\n",
       "19            1.0       0.0  0.857     0.000    0.625\n",
       "20            0.0       1.0  0.000     2.499    1.620\n",
       "21            1.0       1.0  0.106     2.511    1.352\n",
       "22            1.0       1.0  0.332     1.744    1.198\n",
       "23            1.0       1.0  0.214     1.658    1.117\n",
       "24            1.0       1.0  0.867     1.231    1.554\n",
       "25            1.0       0.0  0.788     0.000    1.135\n",
       "26            1.0       1.0  0.483     1.167    0.639\n",
       "27            1.0       1.0  0.834     1.212    1.243\n",
       "28            1.0       1.0  0.720     0.828    0.924\n",
       "29            1.0       1.0  0.697     1.532    1.032"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_parameter_table(A=early_A, Q=early_Q, delta=early_delta, caption=\"CSEDM: Early Item Parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[H]\n",
      "\\centering\n",
      "\\caption{CSEDM: Late Item Parameters, n=367}\n",
      "\\begin{tabular}{lrrrrr}\n",
      "\\toprule\n",
      "\\textbf{Param.} & \\multicolumn{2}{l}{Q} & \\multicolumn{2}{l}{A} & $\\Delta$ \\\\\n",
      "\\textbf{Latent dim.} & Logic & Iteration &  Logic & \\multicolumn{2}{l}{Iteration} \\\\\n",
      "\\midrule\n",
      "\\textbf{0 } &   0.0 &       1.0 &  0.000 &     0.444 &    0.981 \\\\\n",
      "\\textbf{1 } &   1.0 &       1.0 &  1.071 &     0.810 &    0.432 \\\\\n",
      "\\textbf{2 } &   1.0 &       0.0 &  1.245 &     0.000 &    1.533 \\\\\n",
      "\\textbf{3 } &   1.0 &       1.0 &  0.980 &     1.467 &   -0.126 \\\\\n",
      "\\textbf{4 } &   1.0 &       1.0 &  1.162 &     0.566 &    0.350 \\\\\n",
      "\\textbf{5 } &   1.0 &       1.0 &  0.599 &     0.813 &    0.163 \\\\\n",
      "\\textbf{6 } &   1.0 &       1.0 &  0.829 &     0.803 &    0.061 \\\\\n",
      "\\textbf{7 } &   1.0 &       1.0 &  0.252 &     1.232 &    0.202 \\\\\n",
      "\\textbf{8 } &   1.0 &       1.0 &  0.806 &     2.338 &   -0.661 \\\\\n",
      "\\textbf{9 } &   1.0 &       1.0 &  0.014 &     1.608 &   -0.826 \\\\\n",
      "\\textbf{10} &   1.0 &       1.0 &  1.176 &     1.328 &   -0.015 \\\\\n",
      "\\textbf{11} &   1.0 &       1.0 &  0.671 &     2.065 &   -0.026 \\\\\n",
      "\\textbf{12} &   1.0 &       1.0 &  0.213 &     1.479 &   -0.220 \\\\\n",
      "\\textbf{13} &   1.0 &       1.0 &  0.563 &     1.159 &    0.211 \\\\\n",
      "\\textbf{14} &   0.0 &       1.0 &  0.000 &     1.158 &    1.038 \\\\\n",
      "\\textbf{15} &   1.0 &       1.0 &  0.040 &     2.028 &    0.005 \\\\\n",
      "\\textbf{16} &   1.0 &       1.0 &  0.390 &     1.724 &   -0.960 \\\\\n",
      "\\textbf{17} &   1.0 &       1.0 &  0.365 &     1.148 &    0.011 \\\\\n",
      "\\textbf{18} &   0.0 &       1.0 &  0.000 &     1.797 &   -0.033 \\\\\n",
      "\\textbf{19} &   0.0 &       1.0 &  0.000 &     1.473 &    0.302 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jesper\\AppData\\Local\\Temp\\ipykernel_17736\\621918068.py:8: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(data.to_latex(bold_rows=True, multicolumn=True,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Param.</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Q</th>\n",
       "      <th colspan=\"2\" halign=\"left\">A</th>\n",
       "      <th>$\\Delta$</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Latent dim.</th>\n",
       "      <th>Logic</th>\n",
       "      <th>Iteration</th>\n",
       "      <th>Logic</th>\n",
       "      <th>Iteration</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.071</td>\n",
       "      <td>0.810</td>\n",
       "      <td>0.432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.245</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.980</td>\n",
       "      <td>1.467</td>\n",
       "      <td>-0.126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.162</td>\n",
       "      <td>0.566</td>\n",
       "      <td>0.350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.813</td>\n",
       "      <td>0.163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.803</td>\n",
       "      <td>0.061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.252</td>\n",
       "      <td>1.232</td>\n",
       "      <td>0.202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.806</td>\n",
       "      <td>2.338</td>\n",
       "      <td>-0.661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.014</td>\n",
       "      <td>1.608</td>\n",
       "      <td>-0.826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.176</td>\n",
       "      <td>1.328</td>\n",
       "      <td>-0.015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.671</td>\n",
       "      <td>2.065</td>\n",
       "      <td>-0.026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.213</td>\n",
       "      <td>1.479</td>\n",
       "      <td>-0.220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.563</td>\n",
       "      <td>1.159</td>\n",
       "      <td>0.211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.158</td>\n",
       "      <td>1.038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.040</td>\n",
       "      <td>2.028</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.390</td>\n",
       "      <td>1.724</td>\n",
       "      <td>-0.960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.365</td>\n",
       "      <td>1.148</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.797</td>\n",
       "      <td>-0.033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.473</td>\n",
       "      <td>0.302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Param.          Q                A           $\\Delta$\n",
       "Latent dim. Logic Iteration  Logic Iteration         \n",
       "0             0.0       1.0  0.000     0.444    0.981\n",
       "1             1.0       1.0  1.071     0.810    0.432\n",
       "2             1.0       0.0  1.245     0.000    1.533\n",
       "3             1.0       1.0  0.980     1.467   -0.126\n",
       "4             1.0       1.0  1.162     0.566    0.350\n",
       "5             1.0       1.0  0.599     0.813    0.163\n",
       "6             1.0       1.0  0.829     0.803    0.061\n",
       "7             1.0       1.0  0.252     1.232    0.202\n",
       "8             1.0       1.0  0.806     2.338   -0.661\n",
       "9             1.0       1.0  0.014     1.608   -0.826\n",
       "10            1.0       1.0  1.176     1.328   -0.015\n",
       "11            1.0       1.0  0.671     2.065   -0.026\n",
       "12            1.0       1.0  0.213     1.479   -0.220\n",
       "13            1.0       1.0  0.563     1.159    0.211\n",
       "14            0.0       1.0  0.000     1.158    1.038\n",
       "15            1.0       1.0  0.040     2.028    0.005\n",
       "16            1.0       1.0  0.390     1.724   -0.960\n",
       "17            1.0       1.0  0.365     1.148    0.011\n",
       "18            0.0       1.0  0.000     1.797   -0.033\n",
       "19            0.0       1.0  0.000     1.473    0.302"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_parameter_table(A=late_A, Q=late_Q, delta=late_delta_shift, caption=\"CSEDM: Late Item Parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[  2.,  13.,  46., 100., 169.,  35.,   2.,   0.,   0.,   0.],\n",
       "        [  1.,   1.,  23.,  38.,  66.,  86., 114.,  29.,   6.,   3.]]),\n",
       " array([-1.05722614, -0.69482363, -0.33242113,  0.02998138,  0.39238389,\n",
       "         0.75478639,  1.1171889 ,  1.47959141,  1.84199392,  2.20439642,\n",
       "         2.56679893]),\n",
       " <a list of 2 BarContainer objects>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(s_hat_shift)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Person Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#early\n",
    "early_covariance = early_model.person_parameters[\"covariance\"]\n",
    "early_mu = np.zeros(2)\n",
    "\n",
    "#late\n",
    "late_covariance = late_model.person_parameters[\"covariance\"]\n",
    "late_mu = np.concatenate((early_mu, mu_shift), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_person_parameters(covariance, mu, latent_description=[\"Logic\", \"Iteration\"], caption=\"\", n=367):\n",
    "    cov_df = pd.DataFrame(covariance)\n",
    "    index = pd.MultiIndex.from_product([['Prior Competence', 'Competence Gain'], latent_description],\n",
    "                        names=['Param.', 'Latent dim.'])\n",
    "    cov_df.columns = index\n",
    "    mu_df = pd.DataFrame(np.expand_dims(mu, axis=1), columns=[(\"Mean\", \"\")])\n",
    "    person_df = pd.concat((cov_df, mu_df), axis=1)\n",
    "    person_df.index = index\n",
    "    person_df = np.round(person_df, 3)\n",
    "    print(person_df.to_latex(bold_rows=True, multicolumn=True, \n",
    "                                caption=caption+\", n={0}\".format(n), \n",
    "                                position=\"H\", escape=False))\n",
    "    return(person_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[H]\n",
      "\\centering\n",
      "\\caption{CSEDM: Person Parameters, n=367}\n",
      "\\begin{tabular}{llrrrrr}\n",
      "\\toprule\n",
      "                & \\textbf{Param.} & \\multicolumn{2}{l}{Prior Competence} & \\multicolumn{2}{l}{Competence Gain} &   Mean \\\\\n",
      "                & \\textbf{Latent dim.} &            Logic & Iteration &           Logic & \\multicolumn{2}{l}{Iteration} \\\\\n",
      "\\textbf{Param.} & \\textbf{Latent dim.} &                  &           &                 &           &        \\\\\n",
      "\\midrule\n",
      "\\textbf{Prior Competence} & \\textbf{Logic} &            1.000 &     0.284 &           0.130 &     0.071 &  0.000 \\\\\n",
      "                & \\textbf{Iteration} &            0.284 &     1.000 &           0.075 &     0.137 &  0.000 \\\\\n",
      "\\textbf{Competence Gain} & \\textbf{Logic} &            0.130 &     0.075 &           0.364 &     0.107 &  0.355 \\\\\n",
      "                & \\textbf{Iteration} &            0.071 &     0.137 &           0.107 &     0.348 &  0.897 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jesper\\AppData\\Local\\Temp\\ipykernel_17736\\1204287124.py:10: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(person_df.to_latex(bold_rows=True, multicolumn=True,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Param.</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Prior Competence</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Competence Gain</th>\n",
       "      <th>Mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Latent dim.</th>\n",
       "      <th>Logic</th>\n",
       "      <th>Iteration</th>\n",
       "      <th>Logic</th>\n",
       "      <th>Iteration</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Param.</th>\n",
       "      <th>Latent dim.</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Prior Competence</th>\n",
       "      <th>Logic</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Iteration</th>\n",
       "      <td>0.284</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Competence Gain</th>\n",
       "      <th>Logic</th>\n",
       "      <td>0.130</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Iteration</th>\n",
       "      <td>0.071</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.348</td>\n",
       "      <td>0.897</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Param.                       Prior Competence           Competence Gain  \\\n",
       "Latent dim.                             Logic Iteration           Logic   \n",
       "Param.           Latent dim.                                              \n",
       "Prior Competence Logic                  1.000     0.284           0.130   \n",
       "                 Iteration              0.284     1.000           0.075   \n",
       "Competence Gain  Logic                  0.130     0.075           0.364   \n",
       "                 Iteration              0.071     0.137           0.107   \n",
       "\n",
       "Param.                                   Mean  \n",
       "Latent dim.                  Iteration         \n",
       "Param.           Latent dim.                   \n",
       "Prior Competence Logic           0.071  0.000  \n",
       "                 Iteration       0.137  0.000  \n",
       "Competence Gain  Logic           0.107  0.355  \n",
       "                 Iteration       0.348  0.897  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_person_parameters(covariance=late_covariance, mu=late_mu, caption=\"CSEDM: Person Parameters\")\n",
    "# TODO: Evtl. Korrelationsmatrix mit Varianz-Spalte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gain Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_mean(df, metric, quantiles=[0.25, 0.5, 0.75, 1]):\n",
    "    mean_df = pd.DataFrame()\n",
    "    metric_quantiles = np.quantile(df[metric], quantiles)\n",
    "    for quantile in metric_quantiles:\n",
    "        quantile_df = df[df[metric] < quantile]\n",
    "        df = df[df[metric] >= quantile]\n",
    "        mean_df = pd.concat((mean_df, pd.DataFrame(quantile_df.mean()).transpose()), axis=0)\n",
    "    mean_df.index = quantiles\n",
    "    return(mean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_df_csedm = pd.DataFrame(np.concatenate((theta_hat, s_hat_shift), axis=1))\n",
    "latent_df_csedm.columns = [\"prior logic\", \"prior iter\", \"logic gain\", \"iter gain\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prior logic</th>\n",
       "      <th>prior iter</th>\n",
       "      <th>logic gain</th>\n",
       "      <th>iter gain</th>\n",
       "      <th>post logic</th>\n",
       "      <th>post iter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>-0.99</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1.02</td>\n",
       "      <td>-0.57</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>-0.23</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>0.32</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.00</th>\n",
       "      <td>1.09</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1.52</td>\n",
       "      <td>1.38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      prior logic  prior iter  logic gain  iter gain  post logic  post iter\n",
       "0.25        -0.99       -0.32        0.42       1.02       -0.57       0.71\n",
       "0.50        -0.23       -0.03        0.33       0.84        0.10       0.81\n",
       "0.75         0.32       -0.07        0.36       0.88        0.68       0.82\n",
       "1.00         1.09        0.44        0.42       0.95        1.52       1.38"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logic_quantile_mean = quantile_mean(latent_df_csedm, metric=\"prior logic\")\n",
    "logic_quantile_mean[\"post logic\"] = logic_quantile_mean[\"prior logic\"] + logic_quantile_mean[\"logic gain\"]\n",
    "logic_quantile_mean[\"post iter\"] = logic_quantile_mean[\"prior iter\"] + logic_quantile_mean[\"iter gain\"]\n",
    "np.round(logic_quantile_mean, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "logic_quantiles = np.quantile(theta_hat[:,0], [0.25, 0.5, 0.75, 1], axis=0)\n",
    "iter_quantiles = np.quantile(theta_hat[:,1], [0.25, 0.5, 0.75, 1], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_logic_quantiles = np.split(theta_hat[np.argsort(theta_hat[:,0]), :], [10, 20])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5578.337926884925"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_model.marginal_response_loglikelihood(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4232.158225155266"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "late_model.marginal_response_loglikelihood(response_data=V, theta=theta_hat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set prediction\n",
    "not finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PATH = os.path.join(\"path_to_dataset/csedm_challenge_dataset/F19_Release_Test_06-28-21/test\")\n",
    "\n",
    "test_ps2 = ProgSnap2Dataset(os.path.join(TEST_PATH, 'Data'))\n",
    "early_test = pd.read_csv(os.path.join(TEST_PATH, 'early.csv'))\n",
    "late_test = pd.read_csv(os.path.join(TEST_PATH, 'late.csv'))\n",
    "\n",
    "\n",
    "#Obtain Matrix of Response-vectors\n",
    "test_U = pd.crosstab(index = early_test[\"SubjectID\"], \n",
    "            columns=[early_test[\"AssignmentID\"], early_test[\"ProblemID\"]], \n",
    "            values=early_test[\"Label\"],\n",
    "            aggfunc=\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1dab142cd35a3cb14e0e2ff57f489714d4e1ed36d350a97cffb0e76179ad7f53"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
